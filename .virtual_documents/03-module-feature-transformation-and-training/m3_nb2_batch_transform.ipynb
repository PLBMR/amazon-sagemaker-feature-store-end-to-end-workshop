import logging
from urllib.parse import urlparse

import boto3
import pandas as pd
import sagemaker
from sagemaker import get_execution_role
from sagemaker.dataset_definition.inputs import (
    AthenaDatasetDefinition,
    DatasetDefinition,
)
from sagemaker.model import Model
from sagemaker.processing import ProcessingInput, ProcessingOutput
from sagemaker.sklearn.processing import SKLearnProcessor
from sagemaker.utils import name_from_base


logger = logging.getLogger("__name__")
logger.setLevel(logging.DEBUG)
logger.addHandler(logging.StreamHandler())


sagemaker_execution_role = get_execution_role()
logger.info(f"Role = {sagemaker_execution_role}")
session = boto3.Session()
sagemaker_session = sagemaker.Session()
sagemaker_client = session.client(service_name="sagemaker")
featurestore_runtime = boto3.client("sagemaker-featurestore-runtime")

feature_store_session = sagemaker.Session(
    boto_session=session,
    sagemaker_client=sagemaker_client,
    sagemaker_featurestore_runtime_client=featurestore_runtime,
)


default_bucket = sagemaker_session.default_bucket()
prefix = "sagemaker-featurestore-workshop"
s3 = session.resource("s3")


# Retreive FG names
get_ipython().run_line_magic("store", " -r customers_feature_group_name")
get_ipython().run_line_magic("store", " -r products_feature_group_name")
get_ipython().run_line_magic("store", " -r orders_feature_group_name")

customers_fg = sagemaker_client.describe_feature_group(
    FeatureGroupName=customers_feature_group_name
)
products_fg = sagemaker_client.describe_feature_group(
    FeatureGroupName=products_feature_group_name
)
orders_fg = sagemaker_client.describe_feature_group(
    FeatureGroupName=orders_feature_group_name
)

database_name = customers_fg["OfflineStoreConfig"]["DataCatalogConfig"]["Database"]
catalog = customers_fg["OfflineStoreConfig"]["DataCatalogConfig"]["Catalog"]

customers_table = customers_fg["OfflineStoreConfig"]["DataCatalogConfig"]["TableName"]
products_table = products_fg["OfflineStoreConfig"]["DataCatalogConfig"]["TableName"]
orders_table = orders_fg["OfflineStoreConfig"]["DataCatalogConfig"]["TableName"]


customers_fg["EventTimeFeatureName"]
customers_fg["RecordIdentifierFeatureName"]


exclude_fsets = [
    "customer_id",
    "product_id",
    "order_id",
    "event_time",
    "purchase_amount",
    "n_days_since_last_purchase",
]
target_fname = "is_reordered"


def generate_fsets(fg_list, exclude_fsets=None, target_fname=None):
    _fg_lst = []
    for _fg in fg_list:
        _fg_tmp = pd.DataFrame(
            Utils.describe_feature_group(_fg["FeatureGroupName"])["FeatureDefinitions"]
        )
        if exclude_fsets:
            _fg_tmp = _fg_tmp[~_fg_tmp.FeatureName.isin(exclude_fsets)]

        _fg_lst.append(_fg_tmp)
    return pd.concat(_fg_lst, ignore_index=True)


fsets_df = generate_fsets([orders_fg, customers_fg, products_fg], exclude_fsets)
features_names = fsets_df.FeatureName.tolist()


batch_transform_columns_string = ",\n    ".join(f'"{c}"' for c in features_names)

customer_uid = customers_fg["RecordIdentifierFeatureName"]
product_uid = products_fg["RecordIdentifierFeatureName"]
order_uid = orders_fg["RecordIdentifierFeatureName"]

customer_et = customers_fg["EventTimeFeatureName"]
product_et = products_fg["EventTimeFeatureName"]
order_et = orders_fg["EventTimeFeatureName"]


query_string = f"""WITH customer_table AS (
    SELECT *,
        dense_rank() OVER (
            PARTITION BY "{customer_uid}"
            ORDER BY "{customer_et}" DESC,
                "api_invocation_time" DESC,
                "write_time" DESC
        ) AS "rank"
    FROM "{customers_table}"
    WHERE NOT "is_deleted"
),
product_table AS (
    SELECT *,
        dense_rank() OVER (
            PARTITION BY "{product_uid}"
            ORDER BY "{product_et}" DESC,
                "api_invocation_time" DESC,
                "write_time" DESC
        ) AS "rank"
    FROM "{products_table}"
    WHERE NOT "is_deleted"
),
order_table AS (
    SELECT *,
        dense_rank() OVER (
            PARTITION BY "{order_uid}"
            ORDER BY "{order_et}" DESC,
                "api_invocation_time" DESC,
                "write_time" DESC
        ) AS "rank"
    FROM "{orders_table}"
    WHERE NOT "is_deleted"
)
SELECT DISTINCT
    "{order_uid}",
    {batch_transform_columns_string}
FROM customer_table,
    product_table,
    order_table
WHERE order_table."customer_id" = customer_table."customer_id"
    AND order_table."product_id" = product_table."product_id"
    AND customer_table."rank" = 1
    AND product_table."rank" = 1
    AND order_table."rank" = 1
"""
print(query_string)


create_batchdata_processor = SKLearnProcessor(
    framework_version="0.23-1",
    role=sagemaker_execution_role,
    instance_type="ml.m5.xlarge",
    instance_count=2,
    base_job_name=f"{prefix}-batch",
    sagemaker_session=sagemaker_session,
)


athena_data_path = "/opt/ml/processing/athena"
athena_output_s3_uri = f"s3://{default_bucket}/{prefix}/athena/data/"
data_sources = [
    ProcessingInput(
        input_name="athena_dataset",
        dataset_definition=DatasetDefinition(
            local_path=athena_data_path,
            data_distribution_type="ShardedByS3Key",
            athena_dataset_definition=AthenaDatasetDefinition(
                catalog=catalog,
                database=database_name,
                query_string=query_string,
                output_s3_uri=athena_output_s3_uri,
                output_format="PARQUET",
            ),
        ),
    )
]


get_ipython().run_cell_magic("writefile", " create_batchdata.py", """import argparse
import uuid
from pathlib import Path

import pandas as pd

# Parse argument variables passed via the CreateDataset processing step
parser = argparse.ArgumentParser()
parser.add_argument("--athena-data", type=str)
args = parser.parse_args()

dataset_path = Path("/opt/ml/processing/output/dataset")
dataset = pd.read_parquet(args.athena_data, engine="pyarrow")

# Write train, test splits to output path
dataset_output_path = Path("/opt/ml/processing/output/dataset")
dataset.to_csv(
    dataset_output_path / f"dataset-{uuid.uuid4()}.csv", index=False, header=False
)""")


destination_s3_path = f"s3://{default_bucket}/{prefix}/{name_from_base('batch')}"
create_batchdata_processor.run(
    code="create_batchdata.py",
    arguments=[
        "--athena-data",
        athena_data_path,
    ],
    inputs=data_sources,
    outputs=[
        ProcessingOutput(
            output_name="batch_transform_data",
            source="/opt/ml/processing/output/dataset",
            destination=destination_s3_path,
        )
    ],
)


get_ipython().run_line_magic("store", " -r xgb_model_data")

container_uri = sagemaker.image_uris.retrieve(
    region=session.region_name,
    framework="xgboost",
    version="1.0-1",
    image_scope="training",
)

xgb_model = Model(
    image_uri=container_uri,
    model_data=xgb_model_data,
    role=sagemaker_execution_role,
    name=name_from_base("fs-workshop-xgboost-model"),
    sagemaker_session=sagemaker_session,
)


xgb_transformer = xgb_model.transformer(instance_count=1, instance_type="ml.m5.xlarge")

# content_type / accept and split_type / assemble_with are required to use IO joining feature
xgb_transformer.assemble_with = "Line"
xgb_transformer.accept = "text/csv"

# start a transform job
xgb_transformer.transform(
    destination_s3_path,
    content_type="text/csv",
    split_type="Line",
    input_filter="$[2:]",
    join_source="Input",
)
xgb_transformer.wait()


s3 = boto3.resource("s3")


def list_s3_files(s3uri):
    parsed_url = urlparse(s3uri)
    bucket = s3.Bucket(parsed_url.netloc)
    prefix = parsed_url.path[1:]
    return [
        dict(bucket_name=k.bucket_name, key=k.key)
        for k in bucket.objects.filter(Prefix=prefix)
    ]


output_file_list = list_s3_files(xgb_transformer.output_path)
output_file_list


pd.read_csv(
    "s3://{bucket_name}/{key}".format(**output_file_list[0]),
    header=None,
)



