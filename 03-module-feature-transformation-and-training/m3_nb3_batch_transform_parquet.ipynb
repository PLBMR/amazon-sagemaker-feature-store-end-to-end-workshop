{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3: Batch Scoring using a pre-trained XGBoost model\n",
    "**This notebook uses the feature set extracted by `module-2` to create a XGBoost based machine learning model for binary classification**\n",
    "\n",
    "**Note:** Please set kernel to `Python 3 (Data Science)` and select instance to `ml.t3.medium`\n",
    "\n",
    "---\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Background](#Background)\n",
    "1. [Setup](#Setup)\n",
    "1. [Prepare test data](#Prepare-test-data)\n",
    "1. [Batch Transform](#Batch-Transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "After the model is trained, if the goal is to generate predictions on a large dataset where minimizing latency isn't a concern, then SageMaker batch transform is the solution. Functionally, batch transform uses the same mechanics as real-time hosting to generate predictions. It requires a web server that takes in HTTP POST requests a single observation, or mini-batch, at a time. However, unlike real-time hosted endpoints which have persistent hardware (instances stay running until you shut them down), batch transform clusters are torn down when the job completes.\n",
    "\n",
    "In this example, we will walk through the steps to prepare the batch test dataset from feature store using processing job and perform batch transform with the test data available on Amazon S3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.dataset_definition.inputs import (\n",
    "    AthenaDatasetDefinition,\n",
    "    DatasetDefinition,\n",
    ")\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from utilities import Utils\n",
    "import uuid\n",
    "import time\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(\"__name__\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Essentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_execution_role = get_execution_role()\n",
    "logger.info(f\"Role = {sagemaker_execution_role}\")\n",
    "session = boto3.Session()\n",
    "sagemaker_session = sagemaker.Session()\n",
    "sagemaker_client = session.client(service_name=\"sagemaker\")\n",
    "\n",
    "\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "prefix = \"sagemaker-featurestore-workshop\"\n",
    "s3 = session.resource(\"s3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare test data for batch transform \n",
    "<!-- job using processing job with *AthenaDatasetDefinition* -->\n",
    "We create the test dataset that we use in our batch transform job using [*AthenaDatasetDefinition*](https://sagemaker.readthedocs.io/en/stable/api/utility/inputs.html#sagemaker.dataset_definition.inputs.AthenaDatasetDefinition) API.\n",
    "We follow the steps below to prepare the test dataset for batch transform job:\n",
    "1. firstly generates the list of feature names that we would like to read from the offline feature store by providing the feature group names as a list and an exclude feature list to the *generate_fsets* function. \n",
    "2. Construct an Athena query to read the data from offline feature store and run a SageMaker processing job to transform the data type to 'text/csv'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Generate the list of features needed from feature store.\n",
    "\n",
    "We use boto3 sagemaker_client to perform `DescribeFeatureGroup` action to describe a FeatureGroup. The response includes information on the creation time, FeatureGroup name, the unique identifier for each FeatureGroup, and more, for more details of the response syntax, please refer to [document here](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_DescribeFeatureGroup.html#API_DescribeFeatureGroup_ResponseSyntax)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retreive FG names\n",
    "%store -r customers_feature_group_name\n",
    "%store -r products_feature_group_name\n",
    "%store -r orders_feature_group_name\n",
    "\n",
    "customers_fg = sagemaker_client.describe_feature_group(\n",
    "    FeatureGroupName=customers_feature_group_name\n",
    ")\n",
    "products_fg = sagemaker_client.describe_feature_group(\n",
    "    FeatureGroupName=products_feature_group_name\n",
    ")\n",
    "orders_fg = sagemaker_client.describe_feature_group(\n",
    "    FeatureGroupName=orders_feature_group_name\n",
    ")\n",
    "\n",
    "database_name = customers_fg[\"OfflineStoreConfig\"][\"DataCatalogConfig\"][\"Database\"]\n",
    "catalog = customers_fg[\"OfflineStoreConfig\"][\"DataCatalogConfig\"][\"Catalog\"]\n",
    "\n",
    "customers_table = customers_fg[\"OfflineStoreConfig\"][\"DataCatalogConfig\"][\"TableName\"]\n",
    "products_table = products_fg[\"OfflineStoreConfig\"][\"DataCatalogConfig\"][\"TableName\"]\n",
    "orders_table = orders_fg[\"OfflineStoreConfig\"][\"DataCatalogConfig\"][\"TableName\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_fsets = [\n",
    "    \"customer_id\",\n",
    "    \"product_id\",\n",
    "    \"order_id\",\n",
    "    \"event_time\",\n",
    "    \"purchase_amount\",\n",
    "    \"n_days_since_last_purchase\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fsets(fg_list, exclude_fsets=None):\n",
    "    _fg_lst = []\n",
    "    for _fg in fg_list:\n",
    "        _fg_tmp = pd.DataFrame(\n",
    "            Utils.describe_feature_group(_fg[\"FeatureGroupName\"])[\"FeatureDefinitions\"]\n",
    "        )\n",
    "        if exclude_fsets:\n",
    "            _fg_tmp = _fg_tmp[~_fg_tmp.FeatureName.isin(exclude_fsets)]\n",
    "\n",
    "        _fg_lst.append(_fg_tmp)\n",
    "    return pd.concat(_fg_lst, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsets_df = generate_fsets([orders_fg, customers_fg, products_fg], exclude_fsets)\n",
    "features_names = fsets_df.FeatureName.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Athena CTAS table query to generate test set for batch job\n",
    "\n",
    "\n",
    "\n",
    "We start by create an Athena query to get the test data from feature store. Note that the first column will be the unique identifier of customer id and the second column is the target value. Note that the query should only take the latest version of any given record that has multiple write times for the same event_time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_transform_columns_string = \",\\n    \".join(f'\"{c}\"' for c in features_names)\n",
    "\n",
    "customer_uid = customers_fg[\"RecordIdentifierFeatureName\"]\n",
    "product_uid = products_fg[\"RecordIdentifierFeatureName\"]\n",
    "order_uid = orders_fg[\"RecordIdentifierFeatureName\"]\n",
    "\n",
    "customer_et = customers_fg[\"EventTimeFeatureName\"]\n",
    "product_et = products_fg[\"EventTimeFeatureName\"]\n",
    "order_et = orders_fg[\"EventTimeFeatureName\"]\n",
    "\n",
    "\n",
    "query_string = f\"\"\"WITH customer_table AS (\n",
    "    SELECT *,\n",
    "        dense_rank() OVER (\n",
    "            PARTITION BY \"{customer_uid}\"\n",
    "            ORDER BY \"{customer_et}\" DESC,\n",
    "                \"api_invocation_time\" DESC,\n",
    "                \"write_time\" DESC\n",
    "        ) AS \"rank\"\n",
    "    FROM \"{customers_table}\"\n",
    "    WHERE NOT \"is_deleted\"\n",
    "),\n",
    "product_table AS (\n",
    "    SELECT *,\n",
    "        dense_rank() OVER (\n",
    "            PARTITION BY \"{product_uid}\"\n",
    "            ORDER BY \"{product_et}\" DESC,\n",
    "                \"api_invocation_time\" DESC,\n",
    "                \"write_time\" DESC\n",
    "        ) AS \"rank\"\n",
    "    FROM \"{products_table}\"\n",
    "    WHERE NOT \"is_deleted\"\n",
    "),\n",
    "order_table AS (\n",
    "    SELECT *,\n",
    "        dense_rank() OVER (\n",
    "            PARTITION BY \"{order_uid}\"\n",
    "            ORDER BY \"{order_et}\" DESC,\n",
    "                \"api_invocation_time\" DESC,\n",
    "                \"write_time\" DESC\n",
    "        ) AS \"rank\"\n",
    "    FROM \"{orders_table}\"\n",
    "    WHERE NOT \"is_deleted\"\n",
    ")\n",
    "SELECT DISTINCT\n",
    "    \"{order_uid}\",\n",
    "    {batch_transform_columns_string}\n",
    "FROM customer_table,\n",
    "    product_table,\n",
    "    order_table\n",
    "WHERE order_table.\"customer_id\" = customer_table.\"customer_id\"\n",
    "    AND order_table.\"product_id\" = product_table.\"product_id\"\n",
    "    AND customer_table.\"rank\" = 1\n",
    "    AND product_table.\"rank\" = 1\n",
    "    AND order_table.\"rank\" = 1\n",
    "\"\"\"\n",
    "# print(query_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_s3_path = f's3://{default_bucket}/{prefix}/athena/data/sagemaker-batch-{time.strftime(\"%Y-%m-%d-%H%M%S\")}/data'\n",
    "table_name = f'{catalog}.{database_name}.sagemaker_tmp_{uuid.uuid4().hex[:8]}'\n",
    "\n",
    "CTAS_query = f\"\"\"CREATE TABLE {table_name} WITH \n",
    "(external_location='{destination_s3_path}', format='PARQUET') \n",
    "AS {query_string}\n",
    "\"\"\"\n",
    "print(CTAS_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "athena = boto3.client('athena')\n",
    "\n",
    "tmp_uri = f's3://{default_bucket}/{prefix}/offline-store/query_results/'\n",
    "print('Running query on  database: ' + database_name)\n",
    "query_execution = athena.start_query_execution(\n",
    "    QueryString=CTAS_query,\n",
    "    QueryExecutionContext={'Database': database_name},\n",
    "    ResultConfiguration={'OutputLocation': tmp_uri}\n",
    ")\n",
    "# wait for the Athena query to complete\n",
    "query_execution_id = query_execution['QueryExecutionId']\n",
    "query_state = athena.get_query_execution(QueryExecutionId=query_execution_id)['QueryExecution']['Status']['State']\n",
    "while (query_state != 'SUCCEEDED' and query_state != 'FAILED'):\n",
    "    sleep(2)\n",
    "    query_state = athena.get_query_execution(QueryExecutionId=query_execution_id)['QueryExecution']['Status']['State']\n",
    "\n",
    "if query_state == 'FAILED':\n",
    "    print(athena.get_query_execution(QueryExecutionId=query_execution_id))\n",
    "    failure_reason = athena.get_query_execution(QueryExecutionId=query_execution_id)['QueryExecution']['Status']['StateChangeReason']\n",
    "    print(failure_reason)\n",
    "else:\n",
    "    !aws s3 ls $destination_s3_path/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In SageMaker Batch Transform, we introduced 3 new attributes - __input_filter__, __join_source__ and __output_filter__. In the below cell, we use the [SageMaker Python SDK](https://github.com/aws/sagemaker-python-sdk) to kick-off several Batch Transform jobs using different configurations of these 3 new attributes. Please refer to [this page](https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform-data-processing.html) to learn more about how to use them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Create a model based on the pre-trained model artifacts on S3\n",
    "Let's first create a model based on the training job from the previous notebook. We can use `describe_training_job` boto3 api call to get the model data uri and the container used for the training job. Please note that the pre-built [XGBoost container](https://github.com/aws/sagemaker-xgboost-container) uses the same container image for training and hosting. However, if you are using other framework containers, such as TensorFlow, PyTorch and etc., the training and inference containers are different. For more details about the the available deep learning containers, please refer to the [github page](https://github.com/aws/deep-learning-containers/blob/master/available_images.md). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r training_jobName\n",
    "\n",
    "from sagemaker.xgboost.model import XGBoostModel\n",
    "\n",
    "training_job_info = sagemaker_client.describe_training_job(\n",
    "    TrainingJobName=training_jobName\n",
    ")\n",
    "xgb_model_data = training_job_info[\"ModelArtifacts\"][\"S3ModelArtifacts\"]\n",
    "\n",
    "xgb_model = XGBoostModel(\n",
    "    source_dir='./code',\n",
    "    entry_point='inference.py',\n",
    "    framework_version='1.0-1',\n",
    "    model_data=xgb_model_data,\n",
    "    role=sagemaker_execution_role,\n",
    "    name=name_from_base(\"fs-workshop-xgboost-model\"),\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_path = f's3://{default_bucket}/{prefix}/batch_output/{xgb_model.name}'\n",
    "\n",
    "xgb_transformer = xgb_model.transformer(\n",
    "    strategy='SingleRecord',\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    max_concurrent_transforms=4,\n",
    "    accept=\"text/csv\",\n",
    "    output_path=output_path\n",
    ")\n",
    "xgb_transformer.transform(\n",
    "    destination_s3_path, content_type=\"application/x-parquet\", #wait=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the output of the Batch Transform job in S3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource(\"s3\")\n",
    "\n",
    "\n",
    "def list_s3_files(s3uri):\n",
    "    parsed_url = urlparse(s3uri)\n",
    "    bucket = s3.Bucket(parsed_url.netloc)\n",
    "    prefix = parsed_url.path[1:]\n",
    "    return [\n",
    "        dict(bucket_name=k.bucket_name, key=k.key)\n",
    "        for k in bucket.objects.filter(Prefix=prefix)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_list = list_s3_files(xgb_transformer.output_path)\n",
    "output_file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\n",
    "    \"s3://{bucket_name}/{key}\".format(**output_file_list[0]),\n",
    "    header=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO\n",
    "# !aws glue delete-table --database-name $database_name --name $table_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
