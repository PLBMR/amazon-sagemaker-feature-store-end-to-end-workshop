{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3: Model Training\n",
    "**This notebook uses the feature set extracted by `module-2` to create a XGBoost based machine learning model for binary classification**\n",
    "\n",
    "**Note:** Please set kernel to `Python 3 (Data Science)` and select instance to `ml.t3.medium`\n",
    "\n",
    "---\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Background](#Background)\n",
    "1. [Setup](#Setup)\n",
    "1. [Load transformed feature set](#Load-transformed-feature-set)\n",
    "1. [Split data](#Split-data)\n",
    "1. [Train a model using SageMaker built-in XgBoost algorithm](#Train-a-model-using-SageMaker-built-in-XgBoost-algorithm)\n",
    "1. [Real time inference using the deployed endpoint](#Real-time-inference-using-the-deployed-endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "In this notebook, we demonstrate how to use the feature set derived in `Module-2` and create a machine learning model for predicting whether a customer will reorder a product or not based on historical records. Given the problem type is supervised binary classification, we will use a SageMaker built-in algorithm XGBoost to design this classifier. Once the model is trained, we will also deploy the trained model as a SageMaker endpoint for real-time inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker import get_execution_role\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "import logging\n",
    "import boto3\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from utilities import Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger('__name__')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Essentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_execution_role = get_execution_role()\n",
    "logger.info(f'Role = {sagemaker_execution_role}')\n",
    "session = boto3.Session()\n",
    "sagemaker_session = sagemaker.Session()\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'sagemaker-featurestore-workshop'\n",
    "s3 = session.resource('s3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load transformed feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/train/transformed.csv')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move column `is_redordered` to be the first column since our training algorithm `XGBoost` expects the target column to be the first column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_column = df.pop('is_reordered')\n",
    "df.insert(0, 'is_reordered', first_column)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data\n",
    "\n",
    "We will shuffle the whole dataset first (df.sample(frac=1, random_state=123)) and then split our data set into the following parts:\n",
    "\n",
    "* 70% - train set,\n",
    "* 20% - validation set,\n",
    "* 10% - test set\n",
    "\n",
    "**Note:**  In the code below, the first element denotes size for train (0.7 = 70%), second element denotes size for test (1-0.9 = 0.1 = 10%) and difference between the two denotes size for validation(1 - [0.7+0.1] = 0.2 = 20%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, validation_df, test_df = np.split(df.sample(frac=1, random_state=123), [int(.7*len(df)), int(.9*len(df))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save split datasets to local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('../data/train/train.csv', index=False)\n",
    "validation_df.to_csv('../data/validation/validation.csv', index=False)\n",
    "test_df.to_csv('../data/test/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy datasets to S3 from local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.Bucket(default_bucket).Object(os.path.join(prefix, 'train/train.csv')).upload_file('../data/train/train.csv')\n",
    "s3.Bucket(default_bucket).Object(os.path.join(prefix, 'validation/validation.csv')).upload_file('../data/validation/validation.csv')\n",
    "s3.Bucket(default_bucket).Object(os.path.join(prefix, 'test/test.csv')).upload_file('../data/test/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Pointers to the uploaded files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_location = 's3://{}/{}/train/'.format(default_bucket, prefix)\n",
    "validation_set_location = 's3://{}/{}/validation/'.format(default_bucket, prefix)\n",
    "test_set_location = 's3://{}/{}/test/'.format(default_bucket, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_pointer = TrainingInput(s3_data=train_set_location, content_type='csv')\n",
    "validation_set_pointer = TrainingInput(s3_data=validation_set_location, content_type='csv')\n",
    "test_set_pointer = TrainingInput(s3_data=test_set_location, content_type='csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(train_set_pointer.__dict__, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a model using SageMaker built-in XgBoost algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container_uri = sagemaker.image_uris.retrieve(region=session.region_name, \n",
    "                                              framework='xgboost', \n",
    "                                              version='1.0-1', \n",
    "                                              image_scope='training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = sagemaker.estimator.Estimator(image_uri=container_uri,\n",
    "                                    role=sagemaker_execution_role, \n",
    "                                    instance_count=2, \n",
    "                                    instance_type='ml.m5.xlarge',\n",
    "                                    output_path='s3://{}/{}/model-artifacts'.format(default_bucket, prefix),\n",
    "                                    sagemaker_session=sagemaker_session,\n",
    "                                    base_job_name='reorder-classifier')\n",
    "\n",
    "xgb.set_hyperparameters(objective='binary:logistic',\n",
    "                        num_round=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xgb.fit({'train': train_set_pointer, 'validation': validation_set_pointer})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Scoring using the trained XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.dataset_definition.inputs import (\n",
    "    AthenaDatasetDefinition,\n",
    "    DatasetDefinition,\n",
    ")\n",
    "\n",
    "region = sagemaker_session.boto_region_name\n",
    "boto_session = boto3.Session(region_name=region)\n",
    "sagemaker_client = boto_session.client(service_name='sagemaker', region_name=region)\n",
    "\n",
    "feature_store_session = sagemaker.Session(boto_session=boto_session, \n",
    "                                          sagemaker_client=sagemaker_client, \n",
    "                                          sagemaker_featurestore_runtime_client=featurestore_runtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option1: Prepare test data for batch transform job using processing job with *AthenaDatasetDefinition*\n",
    "The following approach firstly generates the list of feature names that we would like to read from the offline feature store by providing the feature group names as a list and a exclude feature list to the *generate_fsets* function. Here we get the list of features that was expected from the model for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retreive FG names\n",
    "%store -r customers_feature_group_name\n",
    "%store -r products_feature_group_name\n",
    "%store -r orders_feature_group_name\n",
    "\n",
    "customers_fg = sagemaker_client.describe_feature_group(FeatureGroupName=customers_feature_group_name)  \n",
    "products_fg = sagemaker_client.describe_feature_group(FeatureGroupName=products_feature_group_name)\n",
    "orders_fg = sagemaker_client.describe_feature_group(FeatureGroupName=orders_feature_group_name)\n",
    "\n",
    "database_name = customers_fg[\"OfflineStoreConfig\"][\"DataCatalogConfig\"][\"Database\"]\n",
    "catalog = customers_fg[\"OfflineStoreConfig\"][\"DataCatalogConfig\"][\"Catalog\"]\n",
    "\n",
    "customers_table = customers_fg[\"OfflineStoreConfig\"][\"DataCatalogConfig\"][\"TableName\"]\n",
    "products_table = products_fg[\"OfflineStoreConfig\"][\"DataCatalogConfig\"][\"TableName\"]\n",
    "orders_table = orders_fg[\"OfflineStoreConfig\"][\"DataCatalogConfig\"][\"TableName\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_fsets = ['customer_id', 'product_id', 'order_id', 'event_time', 'purchase_amount', 'n_days_since_last_purchase']\n",
    "target_fname = 'is_reordered'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fsets(fg_list, exclude_fsets=None, target_fname=None):\n",
    "    _fg_lst = []\n",
    "    for _fg in fg_list:\n",
    "        _fg_tmp = pd.DataFrame(Utils.describe_feature_group(_fg['FeatureGroupName'])['FeatureDefinitions'])\n",
    "        if exclude_fsets:\n",
    "            _fg_tmp = _fg_tmp[~_fg_tmp.FeatureName.isin(exclude_fsets)]\n",
    "            \n",
    "        _fg_lst.append(_fg_tmp)\n",
    "    return pd.concat(_fg_lst, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsets_df = generate_fsets([orders_fg, customers_fg, products_fg], exclude_fsets)\n",
    "features_names = fsets_df.FeatureName.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_transform_columns_string = \",\".join(f'\"{c}\"' for c in features_names)\n",
    "batch_transform_columns_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_string = f'SELECT {batch_transform_columns_string} FROM \"{customers_table}\", \"{products_table}\", \"{orders_table}\" ' \\\n",
    "               f'WHERE (\"{orders_table}\".\"customer_id\" = \"{customers_table}\".\"customer_id\") ' \\\n",
    "               f'AND (\"{orders_table}\".\"product_id\" = \"{products_table}\".\"product_id\")'\n",
    "query_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_batchdata_processor = SKLearnProcessor(\n",
    "    framework_version=\"0.23-1\",\n",
    "    role=sagemaker_execution_role,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=1,\n",
    "    base_job_name=f\"{prefix}-batch\",\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "athena_data_path = \"/opt/ml/processing/athena\"\n",
    "data_sources = []\n",
    "athena_output_s3_uri = f\"s3://{default_bucket}/{prefix}/athena/data/\"\n",
    "data_sources.append(\n",
    "    ProcessingInput(\n",
    "        input_name=\"athena_dataset\",\n",
    "        dataset_definition=DatasetDefinition(\n",
    "            local_path=athena_data_path,\n",
    "            data_distribution_type=\"FullyReplicated\",\n",
    "            athena_dataset_definition=AthenaDatasetDefinition(\n",
    "                catalog=catalog,\n",
    "                database=database_name,\n",
    "                query_string=query_string,\n",
    "                output_s3_uri=athena_output_s3_uri,\n",
    "                output_format=\"PARQUET\",\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile create_batchdata.py\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Parse argument variables passed via the CreateDataset processing step\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--athena-data\", type=str)\n",
    "args = parser.parse_args()\n",
    "\n",
    "dataset_path = Path(\"/opt/ml/processing/output/dataset\")\n",
    "dataset = pd.read_parquet(args.athena_data, engine=\"pyarrow\")\n",
    "\n",
    "# Write train, test splits to output path\n",
    "dataset_output_path = Path(\"/opt/ml/processing/output/dataset\")\n",
    "dataset.to_csv(dataset_output_path / \"dataset.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_s3_path = f\"s3://{default_bucket}/{prefix}/batch\"\n",
    "create_batchdata_processor.run(\n",
    "    code='create_batchdata.py',\n",
    "    arguments=[\n",
    "        \"--athena-data\",\n",
    "        athena_data_path,\n",
    "    ],\n",
    "    inputs=data_sources,\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name=\"batch_transform_data\",\n",
    "            source=\"/opt/ml/processing/output/dataset\",\n",
    "            destination=destination_s3_path,\n",
    "        )\n",
    "    ],\n",
    "        \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option2: Use feature sets Utility function to extract features from offline store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Transform\n",
    "\n",
    "\n",
    "In SageMaker Batch Transform, we introduced 3 new attributes - __input_filter__, __join_source__ and __output_filter__. In the below cell, we use the [SageMaker Python SDK](https://github.com/aws/sagemaker-python-sdk) to kick-off several Batch Transform jobs using different configurations of these 3 new attributes. Please refer to [this page](https://docs.aws.amazon.com/sagemaker/latest/dg/batch-transform-data-processing.html) to learn more about how to use them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Create a model based on the pretrained model artifacts on s3\n",
    "Let's first create a model based on the training job finished in the first section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "from time import gmtime, strftime\n",
    "xgb_model = Model(\n",
    "    image_uri=container_uri,\n",
    "    model_data=xgb.model_data,\n",
    "    role=sagemaker_execution_role,\n",
    "    name=\"fs-workshop-xgboost-model-\" + strftime(\"%Y-%m-%d-%H-%M\", gmtime()),\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Join the input and the prediction results \n",
    "Now, let's associate the prediction results with their corresponding input records. We can also use the __input_filter__ to exclude the ID column easily and there's no need to have a separate file in S3.\n",
    "\n",
    "* Set __input_filter__ to \"$[1:]\": indicates that we are excluding column 0 (the 'ID') before processing the inferences and keeping everything from column 1 to the last column (all the features or predictors)  \n",
    "  \n",
    "  \n",
    "* Set __join_source__ to \"Input\": indicates our desire to join the input data with the inference results  \n",
    "\n",
    "* Leave __output_filter__ to default ('$'), indicating that the joined input and inference results be will saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_transformer = xgb_model.transformer(instance_count=1, instance_type=\"ml.m5.xlarge\")\n",
    "\n",
    "# content_type / accept and split_type / assemble_with are required to use IO joining feature\n",
    "xgb_transformer.assemble_with = \"Line\"\n",
    "xgb_transformer.accept = \"text/csv\"\n",
    "\n",
    "# start a transform job\n",
    "xgb_transformer.transform(destination_s3_path, \n",
    "                         content_type=\"text/csv\", \n",
    "                         split_type=\"Line\",\n",
    "                         input_filter=\"$[1:]\",\n",
    "                         join_source=\"Input\",\n",
    "                        )\n",
    "xgb_transformer.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the output of the Batch Transform job in S3. It should show the list of trips identified by their original feature columns and their corresponding predicted trip fares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import io\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "\n",
    "def get_csv_output_from_s3(s3uri, file_name):\n",
    "    parsed_url = urlparse(s3uri)\n",
    "    bucket_name = parsed_url.netloc\n",
    "    prefix = parsed_url.path[1:]\n",
    "    s3 = boto3.resource(\"s3\")\n",
    "    obj = s3.Object(bucket_name, \"{}/{}\".format(prefix, file_name))\n",
    "    return obj.get()[\"Body\"].read().decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = get_csv_output_from_s3(xgb_transformer.output_path, 'dataset.csv.out')\n",
    "output_df.split('\\n')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
